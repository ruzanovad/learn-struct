{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42b25b2a",
   "metadata": {},
   "source": [
    "\n",
    "# Data Preparation and Clustering Analysis\n",
    "\n",
    "This notebook demonstrates how to collect, preprocess, and analyze text data for clustering purposes. \n",
    "We will start by gathering data from a specified directory, followed by preprocessing the text, and finally applying different clustering algorithms. \n",
    "We will also compare the performance of various clustering methods.\n",
    "\n",
    "## Sections Overview\n",
    "1. **Data Collection**: Walk through directories to collect text data.\n",
    "2. **Data Preprocessing**: Clean and prepare the text data.\n",
    "3. **Clustering**: Apply different clustering algorithms and visualize the results.\n",
    "4. **Comparison**: Compare the performance of clustering algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml._yaml\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.cluster import DBSCAN, Birch\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm  # Importing tqdm for progress bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"/home/kaiser/work/repos/obsidian\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa68e6f4",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Data Collection\n",
    "\n",
    "In this section, we traverse through the directories to collect text files. \n",
    "We will extract relevant information and store it in a DataFrame for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_in_data(rootdir=path):\n",
    "    for folder, _, files in os.walk(rootdir):\n",
    "        print(\"visited\", folder)\n",
    "        for filename in files:\n",
    "            print(\"visited file\", filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_in_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "\n",
    "def walk_in_data_and_add(rootdir=path):\n",
    "    rows_list = []\n",
    "\n",
    "    for folder, dirnames, files in os.walk(rootdir):\n",
    "        for filename in files:\n",
    "            if filename[-2:] != \"md\":\n",
    "                continue\n",
    "            with open(folder + \"/\" + filename, \"r\") as f:\n",
    "                dict = {}\n",
    "                dict.update({\"directory\": folder})\n",
    "                dict.update({\"name\": \".\".join(filename.split(\".\")[:-1])})\n",
    "                dict.update({\"extension\": filename.split(\".\")[-1]})\n",
    "                dict.update({\"text\": f.read()})\n",
    "\n",
    "                rows_list.append(dict)\n",
    "    return pd.DataFrame(rows_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = walk_in_data_and_add()\n",
    "df.to_csv(\"data/first.csv\", index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5631e27",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Data Preprocessing\n",
    "\n",
    "Here we preprocess the text data, which includes removing unnecessary characters, handling YAML front matter, and vectorizing the text data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"text\"].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's detect files that contain YAML front matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_yaml = df[\n",
    "    df[\"text\"].str.contains(r\"(?s)^---\\s*\\n(.*?)\\n---\\s*(\\n|$)\", regex=True)\n",
    "]\n",
    "\n",
    "df_without_yaml = df[~df.isin(df_with_yaml)]\n",
    "df_with_yaml = df_with_yaml.reset_index(drop=True)\n",
    "df_with_yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to extract front matter to separate column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_yaml[\"yaml_content\"] = df_with_yaml[\"text\"].str.extract(\n",
    "    r\"(?s)^---\\s*\\n(.*?)\\n---\\s*(\\n|$)\", expand=False\n",
    ")[0]\n",
    "df_with_yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks that dict column will be painful to use for ML algorithms. It is essential to extract features using `DictVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def erase_yaml(row):\n",
    "    len_of_yaml = 1 + len(row[\"yaml_content\"]) + 8  # --- symbols + \\n\n",
    "    row[\"text\"] = row[\"text\"][len_of_yaml:]\n",
    "    return row\n",
    "\n",
    "\n",
    "df_with_yaml = df_with_yaml.apply(erase_yaml, axis=\"columns\")\n",
    "df_with_yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_yaml(row):\n",
    "    try:\n",
    "        row = yaml.safe_load(row) if pd.notnull(row) else None\n",
    "    except yaml.constructor.ConstructorError:\n",
    "        return \"{}\"\n",
    "\n",
    "    if row == None:\n",
    "        return row\n",
    "    for key in row.keys():\n",
    "        if isinstance(row[key], list) and len(row[key]) == 1:\n",
    "            row[key] = row[key][0]\n",
    "    return row\n",
    "\n",
    "\n",
    "df_with_yaml[\"yaml_content\"] = df_with_yaml[\"yaml_content\"].apply(preprocess_yaml)\n",
    "df_with_yaml[\"yaml_content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_cloud(text, ngram_range=(1, 1)):\n",
    "    vec = TfidfVectorizer(ngram_range=ngram_range)\n",
    "    X = vec.fit_transform(text)\n",
    "    words_tfidf = dict(\n",
    "        zip(vec.get_feature_names_out(), X.sum(axis=0).A1)\n",
    "    )  # np.asarray(X.sum(axis=0)).ravel()\n",
    "    wordCloud = WordCloud(\n",
    "        width=2000, height=2000, random_state=42, background_color=\"white\"\n",
    "    ).generate_from_frequencies(words_tfidf)\n",
    "\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(wordCloud, interpolation=\"bilinear\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A bit of visualization (`text` column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_word_cloud(df_with_yaml[\"text\"], (1, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting YAML information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_yaml_content = pd.json_normalize(df_with_yaml[\"yaml_content\"]).fillna(\"\")\n",
    "normalized_yaml_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These columns are redundant and are not valuable for data analysis. So just drop them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_yaml_content.drop(\n",
    "    columns=[\n",
    "        \"sr-due\",\n",
    "        \"sr-interval\",\n",
    "        \"sr-ease\",\n",
    "        \"excalidraw-plugin\",\n",
    "        \"complexity\",\n",
    "        \"cssclasses\",\n",
    "    ],\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yaml = pd.concat([df_with_yaml, normalized_yaml_content], axis=1).drop(\n",
    "    columns=[\"yaml_content\", \"extension\"]\n",
    ")\n",
    "df_yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yaml.loc[:, \"aliases\"] = df_yaml[\"aliases\"].astype(\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yaml.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform `date` column dtype to `datetime64`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yaml[\"date\"] = pd.to_datetime(df_yaml[\"date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's gather unique tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tags = set()\n",
    "for x in df_yaml[\"tags\"].str.replace(\"[\", \"\").str.replace(\"]\", \"\").str.split(\", \"):\n",
    "    if type(x) != float:\n",
    "        for y in x:\n",
    "            unique_tags.add(y)\n",
    "unique_tags.remove(\"\")\n",
    "unique_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in unique_tags:\n",
    "    df_yaml[\"tag_\" + tag] = df_yaml[\"tags\"].apply(lambda x: tag in x)\n",
    "\n",
    "df_yaml.drop(columns=[\"tags\"], inplace=True)\n",
    "df_yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do the same procedure with `aliases` is not needed - we will process this column with TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_columns = df_yaml.columns.drop(\"date\")\n",
    "datetime_columns = pd.Index([\"date\"])\n",
    "object_columns, datetime_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining with files without YAML frontmatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have totally forgot about documents without yaml frontmatter (actually, they could contain some tags, but in the document using \"#notation\"). Adding them to our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined = pd.merge(df, df_yaml, \"left\", [\"directory\", \"name\"])\n",
    "df_joined[\"text\"] = df_joined[\"text_y\"].fillna(df_joined[\"text_x\"])\n",
    "df_joined.drop(columns=[\"text_x\", \"text_y\"], inplace=True)\n",
    "\n",
    "\n",
    "def fill_na_custom(series):\n",
    "    # print(series.name[:3])\n",
    "    if series.name[:3] == \"tag\":\n",
    "        return series.fillna(False)\n",
    "    elif series.dtype == \"float64\":  # Numeric columns\n",
    "        return series.fillna(0)\n",
    "    elif series.dtype == \"object\":  # String columns\n",
    "        return series.fillna(\"missing\")\n",
    "    elif series.dtype == \"datetime64[ns]\":  # Datetime columns\n",
    "        return series.fillna(pd.Timestamp(\"2024-01-01\"))\n",
    "    else:\n",
    "        return series.fillna(\"other\")  # Default fill for other types\n",
    "\n",
    "\n",
    "# Apply the custom fill logic\n",
    "df_joined = df_joined.apply(fill_na_custom)\n",
    "\n",
    "df_joined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK for joined Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words([\"english\", \"russian\"]))\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    # Replace slashes with spaces\n",
    "    text = text.replace(\"/\", \" \")\n",
    "\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text.lower())\n",
    "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "\n",
    "text_columns = [\"text\", \"aliases\", \"link\", \"directory\", \"name\"]\n",
    "tqdm.pandas()  # Initialize tqdm for pandas\n",
    "for column in text_columns:\n",
    "    df_joined[\"text\"] = df_joined[\"text\"].progress_apply(clean_text)\n",
    "    df_joined[\"aliases\"] = df_joined[\"aliases\"].progress_apply(clean_text)\n",
    "    df_joined[\"link\"] = df_joined[\"link\"].progress_apply(clean_text)\n",
    "    df_joined[\"directory\"] = df_joined[\"directory\"].progress_apply(clean_text)\n",
    "    df_joined[\"name\"] = df_joined[\"name\"].progress_apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_word_cloud(df_joined[\"text\"], (2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"text_directory\", TfidfVectorizer(), \"directory\"),\n",
    "        (\"text_name\", TfidfVectorizer(), \"name\"),\n",
    "        (\"text_text\", TfidfVectorizer(), \"text\"),\n",
    "        (\"text_aliases\", TfidfVectorizer(), \"aliases\"),\n",
    "        (\"text_link\", TfidfVectorizer(), \"link\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Apply transformations\n",
    "transformed_data = pd.DataFrame(preprocessor.fit_transform(df_joined).toarray())\n",
    "transformed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e522bddd",
   "metadata": {},
   "source": [
    "## 3. Clustering & Comparison\n",
    "\n",
    "We will apply different clustering algorithms like DBSCAN and Birch to the preprocessed text data. Finally, we compare the results of different clustering algorithms to evaluate their performance on our dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(transformed_data)\n",
    "\n",
    "nrows = 2\n",
    "ncols = 5\n",
    "\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(25, 10))\n",
    "for i, eps in enumerate(np.linspace(1, 3, nrows * ncols)):\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=10)\n",
    "    clusters = dbscan.fit_predict(transformed_data)\n",
    "\n",
    "    row = i // ncols\n",
    "    col = i % ncols\n",
    "\n",
    "    scatter = axes[row, col].scatter(\n",
    "        X_pca[:, 0], X_pca[:, 1], c=clusters, cmap=\"plasma\"\n",
    "    )\n",
    "    axes[row, col].set_title(f\"DBSCAN eps = {eps:.2f}\")\n",
    "    axes[row, col].set_xlabel(\"PCA Component 1\")\n",
    "    axes[row, col].set_ylabel(\"PCA Component 2\")\n",
    "\n",
    "    # Get unique cluster labels\n",
    "    unique_labels = np.unique(clusters)\n",
    "\n",
    "    # Create legend handles\n",
    "    handles = [\n",
    "        Patch(color=scatter.cmap(scatter.norm(label)), label=f\"Cluster {label}\")\n",
    "        for label in unique_labels\n",
    "    ]\n",
    "\n",
    "    # Add the legend to the plot\n",
    "    axes[row, col].legend(handles=handles, title=\"Clusters\", loc=\"upper right\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Birch\n",
    "The BIRCH (**Balanced Iterative Reducing and Clustering using Hierarchies**) algorithm is a hierarchical clustering method designed to efficiently cluster large (not our case) datasets. BIRCH incrementally builds a tree-like data structure called the Clustering Feature Tree (CF Tree), which summarizes the dataset. This structure allows BIRCH to handle large datasets effectively, making it suitable for scenarios where memory efficiency and scalability are important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(transformed_data)\n",
    "\n",
    "nrows = 3\n",
    "ncols = 5\n",
    "\n",
    "thresholds = np.linspace(0.1, 1.6, ncols)\n",
    "n_clusters = [2, 5, 8]\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(25, 20))\n",
    "for i in range(nrows * ncols):\n",
    "    row = i // ncols\n",
    "    col = i % ncols\n",
    "\n",
    "    dbscan = Birch(threshold=thresholds[col], n_clusters=n_clusters[row])\n",
    "    clusters = dbscan.fit_predict(transformed_data)\n",
    "\n",
    "    scatter = axes[row, col].scatter(\n",
    "        X_pca[:, 0], X_pca[:, 1], c=clusters, cmap=\"plasma\"\n",
    "    )\n",
    "    axes[row, col].set_title(f\"Birch threshold = {thresholds[col]:.2f}, num of clusters = {n_clusters[row]}\")\n",
    "    axes[row, col].set_xlabel(\"PCA Component 1\")\n",
    "    axes[row, col].set_ylabel(\"PCA Component 2\")\n",
    "\n",
    "    # Get unique cluster labels\n",
    "    unique_labels = np.unique(clusters)\n",
    "\n",
    "    # Create legend handles\n",
    "    handles = [\n",
    "        Patch(color=scatter.cmap(scatter.norm(label)), label=f\"Cluster {label}\")\n",
    "        for label in unique_labels\n",
    "    ]\n",
    "\n",
    "    # Add the legend to the plot\n",
    "    axes[row, col].legend(handles=handles, title=\"Clusters\", loc=\"upper right\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like Birch is the best clustering algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_clustering_algo = Birch(threshold=0.1, n_clusters=2)\n",
    "df_joined[\"birch_cluster\"] = best_clustering_algo.fit_predict(transformed_data)\n",
    "df_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined.query(\"birch_cluster == 1\")[\"aliases\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined.query(\"birch_cluster == 0\")[\"aliases\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoops, it seems that the clusters were separated by presence of YAML frontmatter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
