{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42b25b2a",
   "metadata": {},
   "source": [
    "\n",
    "# Data Preparation and Clustering Analysis\n",
    "\n",
    "This notebook demonstrates how to collect, preprocess, and analyze text data for clustering purposes. \n",
    "We will start by gathering data from a specified directory, followed by preprocessing the text, and finally applying different clustering algorithms. \n",
    "We will also compare the performance of various clustering methods.\n",
    "\n",
    "## Sections Overview\n",
    "1. **Data Collection**: Walk through directories to collect text data.\n",
    "2. **Data Preprocessing**: Clean and prepare the text data.\n",
    "3. **Clustering**: Apply different clustering algorithms and visualize the results.\n",
    "4. **Comparison**: Compare the performance of clustering algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import yaml\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import yaml._yaml\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.cluster import DBSCAN, Birch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"/home/kaiser/work/repos/obsidian\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa68e6f4",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Data Collection\n",
    "\n",
    "In this section, we traverse through the directories to collect text files. \n",
    "We will extract relevant information and store it in a DataFrame for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_in_data(rootdir=path):\n",
    "    for folder, _, files in os.walk(rootdir):\n",
    "        print(\"visited\", folder)\n",
    "        for filename in files:\n",
    "            print(\"visited file\", filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_in_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "\n",
    "def walk_in_data_and_add(rootdir=path):\n",
    "    rows_list = []\n",
    "\n",
    "    for folder, dirnames, files in os.walk(rootdir):\n",
    "        for filename in files:\n",
    "            if filename[-2:] != \"md\":\n",
    "                continue\n",
    "            with open(folder + \"/\" + filename, \"r\") as f:\n",
    "                dict = {}\n",
    "                dict.update({\"directory\": folder})\n",
    "                dict.update({\"name\": \".\".join(filename.split(\".\")[:-1])})\n",
    "                dict.update({\"extension\": filename.split(\".\")[-1]})\n",
    "                dict.update({\"text\": f.read()})\n",
    "\n",
    "                rows_list.append(dict)\n",
    "    return pd.DataFrame(rows_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = walk_in_data_and_add()\n",
    "df.to_csv(\"data/first.csv\", index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5631e27",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Data Preprocessing\n",
    "\n",
    "Here we preprocess the text data, which includes removing unnecessary characters, handling YAML front matter, and vectorizing the text data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"text\"].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's detect files that contain YAML front matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_yaml = df[\n",
    "    df[\"text\"].str.contains(r\"(?s)^---\\s*\\n(.*?)\\n---\\s*(\\n|$)\", regex=True)\n",
    "].reset_index(drop=True)\n",
    "df_with_yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to extract front matter to separate column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_yaml[\"yaml_content\"] = df_with_yaml[\"text\"].str.extract(\n",
    "    r\"(?s)^---\\s*\\n(.*?)\\n---\\s*(\\n|$)\", expand=False\n",
    ")[0]\n",
    "df_with_yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks that dict column will be painful to use for ML algorithms. It is essential to extract features using `DictVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def erase_yaml(row):\n",
    "    len_of_yaml = 1 + len(row[\"yaml_content\"]) + 8  # --- symbols + \\n\n",
    "    row[\"text\"] = row[\"text\"][len_of_yaml:]\n",
    "    return row\n",
    "\n",
    "\n",
    "df_with_yaml = df_with_yaml.apply(erase_yaml, axis=\"columns\")\n",
    "df_with_yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_yaml(row):\n",
    "    try:\n",
    "        row = yaml.safe_load(row) if pd.notnull(row) else None\n",
    "    except yaml.constructor.ConstructorError:\n",
    "        return \"{}\"\n",
    "    \n",
    "    if row == None:\n",
    "        return row\n",
    "    for key in row.keys():\n",
    "        if isinstance(row[key], list) and len(row[key]) == 1:\n",
    "            row[key] = row[key][0]\n",
    "    return row\n",
    "\n",
    "\n",
    "df_with_yaml[\"yaml_content\"] = df_with_yaml[\"yaml_content\"].apply(preprocess_yaml)\n",
    "df_with_yaml[\"yaml_content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_cloud(text, ngram_range=(1, 1)):\n",
    "    vec = TfidfVectorizer(ngram_range=ngram_range)\n",
    "    X = vec.fit_transform(text)\n",
    "    words_tfidf = dict(zip(vec.get_feature_names_out(), X.sum(axis=0).A1)) # np.asarray(X.sum(axis=0)).ravel()\n",
    "    wordCloud = WordCloud(\n",
    "        width=2000, height=2000, random_state=42, background_color=\"white\"\n",
    "    ).generate_from_frequencies(words_tfidf)\n",
    "\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(wordCloud, interpolation=\"bilinear\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A bit of visualization (`text` column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_word_cloud(df_with_yaml['text'], (1, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_yaml_content = pd.json_normalize(df_with_yaml[\"yaml_content\"]).fillna(\"\")\n",
    "normalized_yaml_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These columns are redundant and are not valuable for data analysis. So just drop them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_yaml_content.drop(columns=[\"sr-due\", \"sr-interval\", \"sr-ease\", \"excalidraw-plugin\", \"complexity\", \"cssclasses\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yaml = pd.concat([df_with_yaml, normalized_yaml_content], axis=1).drop(columns=[\"yaml_content\", \"extension\"])\n",
    "df_yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yaml.loc[:, \"aliases\"] = df_yaml[\"aliases\"].astype(\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yaml.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform `date` column dtype to `datetime64`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yaml[\"date\"] = pd.to_datetime(df_yaml[\"date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's gather unique tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tags = set()\n",
    "for x in df_yaml[\"tags\"].str.replace(\"[\", \"\").str.replace(\"]\", \"\").str.split(\", \"):\n",
    "    if type(x) != float:\n",
    "        for y in x:\n",
    "            unique_tags.add(y)\n",
    "unique_tags.remove(\"\")\n",
    "unique_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in unique_tags:\n",
    "    df_yaml[\"tag_\" + tag] = df_yaml[\"tags\"].apply(\n",
    "        lambda x: tag in x\n",
    "    )\n",
    "\n",
    "df_yaml.drop(columns=[\"tags\"], inplace=True)\n",
    "df_yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do the same procedure with `aliases` is not needed - we will process this column with TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_columns = df_yaml.columns.drop(\"date\")\n",
    "datetime_columns = pd.Index([\"date\"])\n",
    "object_columns, datetime_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text_directory', TfidfVectorizer(), 'directory'),\n",
    "        ('text_name', TfidfVectorizer(), 'name'),\n",
    "        ('text_text', TfidfVectorizer(), 'text'),\n",
    "        ('text_aliases', TfidfVectorizer(), 'aliases'),\n",
    "        ('text_link', TfidfVectorizer(), 'link'),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Apply transformations\n",
    "transformed_data = pd.DataFrame(preprocessor.fit_transform(df_yaml).toarray())\n",
    "transformed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e522bddd",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Clustering\n",
    "\n",
    "We will apply different clustering algorithms like DBSCAN and Birch to the preprocessed text data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=2)\n",
    "\n",
    "dbscan.fit(transformed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yaml['cluster'] = dbscan.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# Reduce dimensions with PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(transformed_data)\n",
    "\n",
    "# Plotting the clusters after PCA\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=df_yaml['cluster'], cmap='plasma')\n",
    "plt.title('DBSCAN Clustering (PCA Reduced)')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.colorbar(label='Cluster Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd54a80b",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Comparison of Clustering Algorithms\n",
    "\n",
    "Finally, we compare the results of different clustering algorithms to evaluate their performance on our dataset.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
